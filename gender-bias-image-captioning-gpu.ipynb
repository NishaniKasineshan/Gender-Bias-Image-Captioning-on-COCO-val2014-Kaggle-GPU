{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11046698,"sourceType":"datasetVersion","datasetId":6881354}],"dockerImageVersionId":30350,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2025-03-16T10:28:38.254305Z","iopub.execute_input":"2025-03-16T10:28:38.254680Z","iopub.status.idle":"2025-03-16T10:28:39.302590Z","shell.execute_reply.started":"2025-03-16T10:28:38.254592Z","shell.execute_reply":"2025-03-16T10:28:39.301461Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Python 3.7.12\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:58:27.459217Z","iopub.execute_input":"2025-03-16T07:58:27.459631Z","iopub.status.idle":"2025-03-16T07:58:27.464311Z","shell.execute_reply.started":"2025-03-16T07:58:27.459593Z","shell.execute_reply":"2025-03-16T07:58:27.463476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\nfrom tqdm.auto import tqdm\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:04.788116Z","iopub.execute_input":"2025-03-16T10:29:04.788799Z","iopub.status.idle":"2025-03-16T10:29:09.499725Z","shell.execute_reply.started":"2025-03-16T10:29:04.788760Z","shell.execute_reply":"2025-03-16T10:29:09.499008Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# # Download caption annotation files\n# annotation_folder = '/annotations/'\n# if not os.path.exists(os.path.abspath('.') + annotation_folder):\n#   annotation_zip = tf.keras.utils.get_file('captions.zip',\n#                                           cache_subdir=os.path.abspath('.'),\n#                                           origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n#                                           extract = True)\n#   annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_val2014.json'\n#   os.remove(annotation_zip)\n\n# # Download image files\n# image_folder = '/val2014/'\n# if not os.path.exists(os.path.abspath('.') + image_folder):\n#   image_zip = tf.keras.utils.get_file('val2014.zip',\n#                                       cache_subdir=os.path.abspath('.'),\n#                                       origin = 'http://images.cocodataset.org/zips/val2014.zip',\n#                                       extract = True)\n#   PATH = os.path.dirname(image_zip) + image_folder\n#   os.remove(image_zip)\n# else:\n#   PATH = os.path.abspath('.') + image_folder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:09.501113Z","iopub.execute_input":"2025-03-16T10:29:09.501686Z","iopub.status.idle":"2025-03-16T10:29:09.507105Z","shell.execute_reply.started":"2025-03-16T10:29:09.501651Z","shell.execute_reply":"2025-03-16T10:29:09.506060Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"annotation_file = '/kaggle/input/coco-val2014-gender-dataset/gender_annotations.json'\n\nPATH = '/kaggle/input/coco-val2014-gender-dataset/genderval2014/genderval2014/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:09.508173Z","iopub.execute_input":"2025-03-16T10:29:09.508495Z","iopub.status.idle":"2025-03-16T10:29:09.521745Z","shell.execute_reply.started":"2025-03-16T10:29:09.508460Z","shell.execute_reply":"2025-03-16T10:29:09.520862Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Read the json file\nwith open(annotation_file, 'r') as f:\n    annotations = json.load(f)\n\n# Store captions and image names in vectors\nall_captions = []\nall_img_name_vector = []\n\nfor annot in annotations['annotations']:\n    caption = '<start> ' + annot['caption'] + ' <end>'\n    image_id = annot['image_id']\n    full_coco_image_path = PATH + 'COCO_val2014_' + '%012d.jpg' % (image_id)\n\n    all_img_name_vector.append(full_coco_image_path)\n    all_captions.append(caption)\n    \n# Shuffle captions and image_names together\n# Set a random state\ntrain_captions, img_name_vector = shuffle(all_captions,\n                                          all_img_name_vector,\n                                          random_state=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:09.523386Z","iopub.execute_input":"2025-03-16T10:29:09.523654Z","iopub.status.idle":"2025-03-16T10:29:10.011719Z","shell.execute_reply.started":"2025-03-16T10:29:09.523631Z","shell.execute_reply":"2025-03-16T10:29:10.011024Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_captions = train_captions[:30000]\nimg_name_vector = img_name_vector[:30000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:10.012923Z","iopub.execute_input":"2025-03-16T10:29:10.013546Z","iopub.status.idle":"2025-03-16T10:29:10.021414Z","shell.execute_reply.started":"2025-03-16T10:29:10.013509Z","shell.execute_reply":"2025-03-16T10:29:10.020528Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"len(train_captions), len(all_captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:57.432574Z","iopub.execute_input":"2025-03-16T10:29:57.433266Z","iopub.status.idle":"2025-03-16T10:29:57.440278Z","shell.execute_reply.started":"2025-03-16T10:29:57.433234Z","shell.execute_reply":"2025-03-16T10:29:57.439345Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(30000, 91502)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:57.697390Z","iopub.execute_input":"2025-03-16T10:29:57.698125Z","iopub.status.idle":"2025-03-16T10:29:57.702801Z","shell.execute_reply.started":"2025-03-16T10:29:57.698094Z","shell.execute_reply":"2025-03-16T10:29:57.701987Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output #(8x8x2048)\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:57.901876Z","iopub.execute_input":"2025-03-16T10:29:57.902250Z","iopub.status.idle":"2025-03-16T10:30:04.646026Z","shell.execute_reply.started":"2025-03-16T10:29:57.902213Z","shell.execute_reply":"2025-03-16T10:30:04.645170Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87916544/87910968 [==============================] - 0s 0us/step\n87924736/87910968 [==============================] - 0s 0us/step\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# !pip install -q tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:30:04.647487Z","iopub.execute_input":"2025-03-16T10:30:04.647783Z","iopub.status.idle":"2025-03-16T10:30:04.652045Z","shell.execute_reply.started":"2025-03-16T10:30:04.647758Z","shell.execute_reply":"2025-03-16T10:30:04.650951Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:30:04.653252Z","iopub.execute_input":"2025-03-16T10:30:04.653696Z","iopub.status.idle":"2025-03-16T10:30:04.662422Z","shell.execute_reply.started":"2025-03-16T10:30:04.653671Z","shell.execute_reply":"2025-03-16T10:30:04.661547Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Get unique images\nencode_train = sorted(set(img_name_vector))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(\n  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:30:04.663918Z","iopub.execute_input":"2025-03-16T10:30:04.664173Z","iopub.status.idle":"2025-03-16T10:30:04.806555Z","shell.execute_reply.started":"2025-03-16T10:30:04.664150Z","shell.execute_reply":"2025-03-16T10:30:04.805584Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"save_dir = \"/kaggle/working/features/\"\nos.makedirs(save_dir, exist_ok=True) \n\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n      file_name = os.path.basename(p.numpy().decode(\"utf-8\"))\n      path_of_feature = os.path.join(save_dir, file_name + \".npy\")\n      np.save(path_of_feature, bf.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:30:04.807762Z","iopub.execute_input":"2025-03-16T10:30:04.808087Z","iopub.status.idle":"2025-03-16T10:32:11.092960Z","shell.execute_reply.started":"2025-03-16T10:30:04.808055Z","shell.execute_reply":"2025-03-16T10:32:11.090642Z"}},"outputs":[{"name":"stderr","text":" 55%|█████▍    | 542/989 [01:12<00:50,  8.85it/s]Cleanup called...\n100%|██████████| 989/989 [02:06<00:00,  7.83it/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def calc_max_length(tensor):\n    return max(len(t) for t in tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:26.242919Z","iopub.execute_input":"2025-03-16T10:32:26.243815Z","iopub.status.idle":"2025-03-16T10:32:26.248279Z","shell.execute_reply.started":"2025-03-16T10:32:26.243780Z","shell.execute_reply":"2025-03-16T10:32:26.247191Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"top_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:26.623448Z","iopub.execute_input":"2025-03-16T10:32:26.624147Z","iopub.status.idle":"2025-03-16T10:32:27.342727Z","shell.execute_reply.started":"2025-03-16T10:32:26.624115Z","shell.execute_reply":"2025-03-16T10:32:27.342032Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:28.477174Z","iopub.execute_input":"2025-03-16T10:32:28.477961Z","iopub.status.idle":"2025-03-16T10:32:28.481963Z","shell.execute_reply.started":"2025-03-16T10:32:28.477928Z","shell.execute_reply":"2025-03-16T10:32:28.481009Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:28.763579Z","iopub.execute_input":"2025-03-16T10:32:28.764277Z","iopub.status.idle":"2025-03-16T10:32:29.263224Z","shell.execute_reply.started":"2025-03-16T10:32:28.764246Z","shell.execute_reply":"2025-03-16T10:32:29.262246Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:29.264964Z","iopub.execute_input":"2025-03-16T10:32:29.265267Z","iopub.status.idle":"2025-03-16T10:32:29.344185Z","shell.execute_reply.started":"2025-03-16T10:32:29.265242Z","shell.execute_reply":"2025-03-16T10:32:29.343316Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:29.345449Z","iopub.execute_input":"2025-03-16T10:32:29.346068Z","iopub.status.idle":"2025-03-16T10:32:29.352887Z","shell.execute_reply.started":"2025-03-16T10:32:29.346031Z","shell.execute_reply":"2025-03-16T10:32:29.351897Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # Create training and validation sets using an 80-20 split\n# img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n#                                                                     cap_vector,\n#                                                                     test_size=0.0333,\n#                                                                     random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:31.533261Z","iopub.execute_input":"2025-03-16T10:32:31.534040Z","iopub.status.idle":"2025-03-16T10:32:31.537657Z","shell.execute_reply.started":"2025-03-16T10:32:31.533989Z","shell.execute_reply":"2025-03-16T10:32:31.536720Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"img_name_train, img_name_temp, cap_train, cap_temp = train_test_split(\n    img_name_vector, cap_vector, test_size=0.30, random_state=0)\n\nimg_name_val, img_name_test, cap_val, cap_test = train_test_split(\n    img_name_temp, cap_temp, test_size=0.3333, random_state=0)  \n\n# Output dataset sizes\nprint(f\"Training set: {len(img_name_train)} samples\")\nprint(f\"Validation set: {len(img_name_val)} samples\")\nprint(f\"Test set: {len(img_name_test)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:32.190893Z","iopub.execute_input":"2025-03-16T10:32:32.191258Z","iopub.status.idle":"2025-03-16T10:32:32.213866Z","shell.execute_reply.started":"2025-03-16T10:32:32.191228Z","shell.execute_reply":"2025-03-16T10:32:32.212825Z"}},"outputs":[{"name":"stdout","text":"Training set: 21000 samples\nValidation set: 6000 samples\nTest set: 3000 samples\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# len(img_name_train), len(img_name_val),len(img_name_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:38.071891Z","iopub.execute_input":"2025-03-16T10:32:38.072703Z","iopub.status.idle":"2025-03-16T10:32:38.076204Z","shell.execute_reply.started":"2025-03-16T10:32:38.072664Z","shell.execute_reply":"2025-03-16T10:32:38.075273Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Feel free to change these parameters according to your system's configuration\n\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = top_k + 1\nnum_steps = len(img_name_train) // BATCH_SIZE\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:38.257886Z","iopub.execute_input":"2025-03-16T10:32:38.258468Z","iopub.status.idle":"2025-03-16T10:32:38.262967Z","shell.execute_reply.started":"2025-03-16T10:32:38.258440Z","shell.execute_reply":"2025-03-16T10:32:38.262063Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name, cap):\n    feature_dir = \"/kaggle/working/features/\"\n    file_name = os.path.basename(img_name.decode('utf-8')) + \".npy\"\n    img_tensor = np.load(os.path.join(feature_dir, file_name))\n    return img_tensor, cap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:38.536104Z","iopub.execute_input":"2025-03-16T10:32:38.536434Z","iopub.status.idle":"2025-03-16T10:32:38.541328Z","shell.execute_reply.started":"2025-03-16T10:32:38.536407Z","shell.execute_reply":"2025-03-16T10:32:38.540353Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def create_dataset(images,captions):\n  dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n\n  # Use map to load the numpy files in parallel\n  dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n  # Shuffle and batch\n  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:41.642839Z","iopub.execute_input":"2025-03-16T10:32:41.643827Z","iopub.status.idle":"2025-03-16T10:32:41.650372Z","shell.execute_reply.started":"2025-03-16T10:32:41.643791Z","shell.execute_reply":"2025-03-16T10:32:41.649582Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# # Use map to load the numpy files in parallel\n# dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n#           map_func, [item1, item2], [tf.float32, tf.int32]),\n#           num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# # Shuffle and batch\n# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n# dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:41.922152Z","iopub.execute_input":"2025-03-16T10:32:41.922498Z","iopub.status.idle":"2025-03-16T10:32:41.926971Z","shell.execute_reply.started":"2025-03-16T10:32:41.922466Z","shell.execute_reply":"2025-03-16T10:32:41.925964Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Creating train, val and test dataset\ntrain_dataset = create_dataset(img_name_train,cap_train)\nval_dataset=create_dataset(img_name_val,cap_val)\ntest_dataset = create_dataset(img_name_test,cap_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:43.202214Z","iopub.execute_input":"2025-03-16T10:32:43.203065Z","iopub.status.idle":"2025-03-16T10:32:43.413001Z","shell.execute_reply.started":"2025-03-16T10:32:43.203004Z","shell.execute_reply":"2025-03-16T10:32:43.412131Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n    \n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n    \n    # score shape == (batch_size, 64, hidden_size)\n    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n    # attention_weights shape == (batch_size, 64, 1)\n    # you get 1 at the last axis because you are applying score to self.V\n    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n    \n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    \n    return context_vector, attention_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:33:11.212167Z","iopub.execute_input":"2025-03-16T10:33:11.212866Z","iopub.status.idle":"2025-03-16T10:33:11.219272Z","shell.execute_reply.started":"2025-03-16T10:33:11.212833Z","shell.execute_reply":"2025-03-16T10:33:11.218425Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:33:42.521644Z","iopub.execute_input":"2025-03-16T10:33:42.522473Z","iopub.status.idle":"2025-03-16T10:33:42.527485Z","shell.execute_reply.started":"2025-03-16T10:33:42.522434Z","shell.execute_reply":"2025-03-16T10:33:42.526549Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:35:50.657314Z","iopub.execute_input":"2025-03-16T10:35:50.658327Z","iopub.status.idle":"2025-03-16T10:35:50.670676Z","shell.execute_reply.started":"2025-03-16T10:35:50.658279Z","shell.execute_reply":"2025-03-16T10:35:50.669507Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"encoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:36:16.457738Z","iopub.execute_input":"2025-03-16T10:36:16.458587Z","iopub.status.idle":"2025-03-16T10:36:16.480620Z","shell.execute_reply.started":"2025-03-16T10:36:16.458552Z","shell.execute_reply":"2025-03-16T10:36:16.479932Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:36:16.826254Z","iopub.execute_input":"2025-03-16T10:36:16.826592Z","iopub.status.idle":"2025-03-16T10:36:16.832525Z","shell.execute_reply.started":"2025-03-16T10:36:16.826552Z","shell.execute_reply":"2025-03-16T10:36:16.831647Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:28.541693Z","iopub.execute_input":"2025-03-16T10:38:28.542386Z","iopub.status.idle":"2025-03-16T10:38:28.547804Z","shell.execute_reply.started":"2025-03-16T10:38:28.542349Z","shell.execute_reply":"2025-03-16T10:38:28.546725Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:28.780274Z","iopub.execute_input":"2025-03-16T10:38:28.780953Z","iopub.status.idle":"2025-03-16T10:38:28.785300Z","shell.execute_reply.started":"2025-03-16T10:38:28.780922Z","shell.execute_reply":"2025-03-16T10:38:28.784447Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:28.974163Z","iopub.execute_input":"2025-03-16T10:38:28.974735Z","iopub.status.idle":"2025-03-16T10:38:28.978609Z","shell.execute_reply.started":"2025-03-16T10:38:28.974706Z","shell.execute_reply":"2025-03-16T10:38:28.977729Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, trainable_variables)\n\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:29.176162Z","iopub.execute_input":"2025-03-16T10:38:29.176435Z","iopub.status.idle":"2025-03-16T10:38:29.184947Z","shell.execute_reply.started":"2025-03-16T10:38:29.176409Z","shell.execute_reply":"2025-03-16T10:38:29.184291Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"val_loss_plot = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:29.372088Z","iopub.execute_input":"2025-03-16T10:38:29.372330Z","iopub.status.idle":"2025-03-16T10:38:29.376209Z","shell.execute_reply.started":"2025-03-16T10:38:29.372308Z","shell.execute_reply":"2025-03-16T10:38:29.375337Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"@tf.function\ndef val_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  features = encoder(img_tensor)\n\n  for i in range(1, target.shape[1]):\n      # passing the features through the decoder\n\n      \n      predictions, hidden, _= decoder(dec_input, features, hidden)\n      # predictions : (64,8329)\n      loss += loss_function(target[:, i], predictions)\n      \n      predicted_id = tf.argmax(predictions[0])\n      dec_input = tf.expand_dims([predicted_id]*target.shape[0], 1)\n \n\n  total_loss = (loss / int(target.shape[1]))\n\n  return loss, total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:29.796393Z","iopub.execute_input":"2025-03-16T10:38:29.797106Z","iopub.status.idle":"2025-03-16T10:38:29.803889Z","shell.execute_reply.started":"2025-03-16T10:38:29.797072Z","shell.execute_reply":"2025-03-16T10:38:29.802920Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"EPOCHS = 30\n\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    total_loss_val = 0\n    for (batch, (img_tensor, target)) in enumerate(val_dataset):\n        batch_loss, v_loss = val_step(img_tensor, target)\n        total_loss_val += v_loss\n    # storing the epoch end loss value to plot later\n    val_loss_plot.append(total_loss_val / num_steps) \n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print ('Epoch {} TrainLoss {:.6f} ValLoss {:.6f}'.format(epoch + 1,(total_loss/num_steps),(total_loss_val/num_steps)))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:38:29.985138Z","iopub.execute_input":"2025-03-16T10:38:29.985440Z","iopub.status.idle":"2025-03-16T11:14:10.712498Z","shell.execute_reply.started":"2025-03-16T10:38:29.985414Z","shell.execute_reply":"2025-03-16T11:14:10.711557Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Batch 0 Loss 1.9113\nEpoch 1 Batch 100 Loss 1.2444\nEpoch 1 Batch 200 Loss 0.9534\nEpoch 1 Batch 300 Loss 0.8858\nEpoch 1 TrainLoss 1.027756 ValLoss 0.381344\nTime taken for 1 epoch 179.42139410972595 sec\n\nEpoch 2 Batch 0 Loss 0.8939\nEpoch 2 Batch 100 Loss 0.8164\nEpoch 2 Batch 200 Loss 0.8290\nEpoch 2 Batch 300 Loss 0.7204\nEpoch 2 TrainLoss 0.786917 ValLoss 0.395521\nTime taken for 1 epoch 68.11884570121765 sec\n\nEpoch 3 Batch 0 Loss 0.6926\nEpoch 3 Batch 100 Loss 0.6895\nEpoch 3 Batch 200 Loss 0.6665\nEpoch 3 Batch 300 Loss 0.7134\nEpoch 3 TrainLoss 0.711844 ValLoss 0.402320\nTime taken for 1 epoch 67.5367522239685 sec\n\nEpoch 4 Batch 0 Loss 0.7079\nEpoch 4 Batch 100 Loss 0.6653\nEpoch 4 Batch 200 Loss 0.6149\nEpoch 4 Batch 300 Loss 0.6398\nEpoch 4 TrainLoss 0.664537 ValLoss 0.402827\nTime taken for 1 epoch 67.78271651268005 sec\n\nEpoch 5 Batch 0 Loss 0.5975\nEpoch 5 Batch 100 Loss 0.6328\nEpoch 5 Batch 200 Loss 0.5880\nEpoch 5 Batch 300 Loss 0.6491\nEpoch 5 TrainLoss 0.627992 ValLoss 0.407366\nTime taken for 1 epoch 67.52722001075745 sec\n\nEpoch 6 Batch 0 Loss 0.6425\nEpoch 6 Batch 100 Loss 0.5312\nEpoch 6 Batch 200 Loss 0.5827\nEpoch 6 Batch 300 Loss 0.5906\nEpoch 6 TrainLoss 0.595020 ValLoss 0.414225\nTime taken for 1 epoch 67.77266478538513 sec\n\nEpoch 7 Batch 0 Loss 0.5801\nEpoch 7 Batch 100 Loss 0.5779\nEpoch 7 Batch 200 Loss 0.5826\nEpoch 7 Batch 300 Loss 0.5710\nEpoch 7 TrainLoss 0.563825 ValLoss 0.423447\nTime taken for 1 epoch 67.7076985836029 sec\n\nEpoch 8 Batch 0 Loss 0.5492\nEpoch 8 Batch 100 Loss 0.4901\nEpoch 8 Batch 200 Loss 0.5454\nEpoch 8 Batch 300 Loss 0.5546\nEpoch 8 TrainLoss 0.533955 ValLoss 0.437397\nTime taken for 1 epoch 67.79678893089294 sec\n\nEpoch 9 Batch 0 Loss 0.5058\nEpoch 9 Batch 100 Loss 0.5148\nEpoch 9 Batch 200 Loss 0.5049\nEpoch 9 Batch 300 Loss 0.4576\nEpoch 9 TrainLoss 0.504773 ValLoss 0.442723\nTime taken for 1 epoch 67.49436020851135 sec\n\nEpoch 10 Batch 0 Loss 0.4970\nEpoch 10 Batch 100 Loss 0.4826\nEpoch 10 Batch 200 Loss 0.4864\nEpoch 10 Batch 300 Loss 0.4052\nEpoch 10 TrainLoss 0.476342 ValLoss 0.457654\nTime taken for 1 epoch 67.67144012451172 sec\n\nEpoch 11 Batch 0 Loss 0.4720\nEpoch 11 Batch 100 Loss 0.4273\nEpoch 11 Batch 200 Loss 0.4103\nEpoch 11 Batch 300 Loss 0.4398\nEpoch 11 TrainLoss 0.449086 ValLoss 0.483707\nTime taken for 1 epoch 67.93590450286865 sec\n\nEpoch 12 Batch 0 Loss 0.4620\nEpoch 12 Batch 100 Loss 0.4293\nEpoch 12 Batch 200 Loss 0.4154\nEpoch 12 Batch 300 Loss 0.3847\nEpoch 12 TrainLoss 0.422295 ValLoss 0.493673\nTime taken for 1 epoch 67.642085313797 sec\n\nEpoch 13 Batch 0 Loss 0.4316\nEpoch 13 Batch 100 Loss 0.4302\nEpoch 13 Batch 200 Loss 0.3572\nEpoch 13 Batch 300 Loss 0.3728\nEpoch 13 TrainLoss 0.394686 ValLoss 0.532817\nTime taken for 1 epoch 67.63763165473938 sec\n\nEpoch 14 Batch 0 Loss 0.4252\nEpoch 14 Batch 100 Loss 0.3858\nEpoch 14 Batch 200 Loss 0.4271\nEpoch 14 Batch 300 Loss 0.3420\nEpoch 14 TrainLoss 0.370162 ValLoss 0.549666\nTime taken for 1 epoch 67.02608895301819 sec\n\nEpoch 15 Batch 0 Loss 0.4167\nEpoch 15 Batch 100 Loss 0.3391\nEpoch 15 Batch 200 Loss 0.3227\nEpoch 15 Batch 300 Loss 0.3271\nEpoch 15 TrainLoss 0.347188 ValLoss 0.545927\nTime taken for 1 epoch 67.47848510742188 sec\n\nEpoch 16 Batch 0 Loss 0.3751\nEpoch 16 Batch 100 Loss 0.3131\nEpoch 16 Batch 200 Loss 0.3383\nEpoch 16 Batch 300 Loss 0.2902\nEpoch 16 TrainLoss 0.326319 ValLoss 0.585059\nTime taken for 1 epoch 67.56121802330017 sec\n\nEpoch 17 Batch 0 Loss 0.3553\nEpoch 17 Batch 100 Loss 0.3011\nEpoch 17 Batch 200 Loss 0.3126\nEpoch 17 Batch 300 Loss 0.2876\nEpoch 17 TrainLoss 0.304448 ValLoss 0.615173\nTime taken for 1 epoch 67.51584935188293 sec\n\nEpoch 18 Batch 0 Loss 0.3103\nEpoch 18 Batch 100 Loss 0.3047\nEpoch 18 Batch 200 Loss 0.2872\nEpoch 18 Batch 300 Loss 0.2563\nEpoch 18 TrainLoss 0.286341 ValLoss 0.630744\nTime taken for 1 epoch 67.17724394798279 sec\n\nEpoch 19 Batch 0 Loss 0.2988\nEpoch 19 Batch 100 Loss 0.2787\nEpoch 19 Batch 200 Loss 0.2540\nEpoch 19 Batch 300 Loss 0.2496\nEpoch 19 TrainLoss 0.267267 ValLoss 0.658487\nTime taken for 1 epoch 67.35366892814636 sec\n\nEpoch 20 Batch 0 Loss 0.3222\nEpoch 20 Batch 100 Loss 0.2444\nEpoch 20 Batch 200 Loss 0.2398\nEpoch 20 Batch 300 Loss 0.2147\nEpoch 20 TrainLoss 0.249810 ValLoss 0.690903\nTime taken for 1 epoch 67.50108623504639 sec\n\nEpoch 21 Batch 0 Loss 0.2708\nEpoch 21 Batch 100 Loss 0.2384\nEpoch 21 Batch 200 Loss 0.2165\nEpoch 21 Batch 300 Loss 0.1979\nEpoch 21 TrainLoss 0.236454 ValLoss 0.701502\nTime taken for 1 epoch 67.88635349273682 sec\n\nEpoch 22 Batch 0 Loss 0.2815\nEpoch 22 Batch 100 Loss 0.2335\nEpoch 22 Batch 200 Loss 0.2191\nEpoch 22 Batch 300 Loss 0.1897\nEpoch 22 TrainLoss 0.224699 ValLoss 0.742182\nTime taken for 1 epoch 67.55049085617065 sec\n\nEpoch 23 Batch 0 Loss 0.2686\nEpoch 23 Batch 100 Loss 0.2077\nEpoch 23 Batch 200 Loss 0.1963\nEpoch 23 Batch 300 Loss 0.1732\nEpoch 23 TrainLoss 0.208350 ValLoss 0.760589\nTime taken for 1 epoch 67.81798696517944 sec\n\nEpoch 24 Batch 0 Loss 0.2585\nEpoch 24 Batch 100 Loss 0.1657\nEpoch 24 Batch 200 Loss 0.1996\nEpoch 24 Batch 300 Loss 0.1777\nEpoch 24 TrainLoss 0.197004 ValLoss 0.770273\nTime taken for 1 epoch 67.47682642936707 sec\n\nEpoch 25 Batch 0 Loss 0.2466\nEpoch 25 Batch 100 Loss 0.1784\nEpoch 25 Batch 200 Loss 0.1688\nEpoch 25 Batch 300 Loss 0.1682\nEpoch 25 TrainLoss 0.188603 ValLoss 0.791508\nTime taken for 1 epoch 67.40822720527649 sec\n\nEpoch 26 Batch 0 Loss 0.2384\nEpoch 26 Batch 100 Loss 0.1938\nEpoch 26 Batch 200 Loss 0.1575\nEpoch 26 Batch 300 Loss 0.1575\nEpoch 26 TrainLoss 0.177305 ValLoss 0.832603\nTime taken for 1 epoch 67.78406286239624 sec\n\nEpoch 27 Batch 0 Loss 0.2433\nEpoch 27 Batch 100 Loss 0.1746\nEpoch 27 Batch 200 Loss 0.1659\nEpoch 27 Batch 300 Loss 0.1613\nEpoch 27 TrainLoss 0.171250 ValLoss 0.859648\nTime taken for 1 epoch 67.64948105812073 sec\n\nEpoch 28 Batch 0 Loss 0.2185\nEpoch 28 Batch 100 Loss 0.1783\nEpoch 28 Batch 200 Loss 0.1496\nEpoch 28 Batch 300 Loss 0.1549\nEpoch 28 TrainLoss 0.157718 ValLoss 0.869039\nTime taken for 1 epoch 68.11119604110718 sec\n\nEpoch 29 Batch 0 Loss 0.1943\nEpoch 29 Batch 100 Loss 0.1327\nEpoch 29 Batch 200 Loss 0.1460\nEpoch 29 Batch 300 Loss 0.1471\nEpoch 29 TrainLoss 0.152928 ValLoss 0.898891\nTime taken for 1 epoch 67.5516881942749 sec\n\nEpoch 30 Batch 0 Loss 0.2142\nEpoch 30 Batch 100 Loss 0.1299\nEpoch 30 Batch 200 Loss 0.1330\nEpoch 30 Batch 300 Loss 0.1410\nEpoch 30 TrainLoss 0.146267 ValLoss 0.909504\nTime taken for 1 epoch 67.82425808906555 sec\n\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:14:19.923049Z","iopub.execute_input":"2025-03-16T11:14:19.923380Z","iopub.status.idle":"2025-03-16T11:14:20.139661Z","shell.execute_reply.started":"2025-03-16T11:14:19.923351Z","shell.execute_reply":"2025-03-16T11:14:20.138768Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":48},{"cell_type":"code","source":"plt.plot(val_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:14:22.876348Z","iopub.execute_input":"2025-03-16T11:14:22.877191Z","iopub.status.idle":"2025-03-16T11:14:22.985986Z","shell.execute_reply.started":"2025-03-16T11:14:22.877155Z","shell.execute_reply":"2025-03-16T11:14:22.984937Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":49},{"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot\ndef evaluate_without_plot(image):\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n    attention_weights_dict = {\"man\": None, \"woman\": None}\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] in attention_weights_dict:\n            attention_weights_dict[tokenizer.index_word[predicted_id]] = attention_weights.numpy()\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result,attention_weights_dict\n    \n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result,attention_weights_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:14:27.036572Z","iopub.execute_input":"2025-03-16T11:14:27.037393Z","iopub.status.idle":"2025-03-16T11:14:27.048634Z","shell.execute_reply.started":"2025-03-16T11:14:27.037358Z","shell.execute_reply":"2025-03-16T11:14:27.047650Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (8, 8))\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:14:29.521816Z","iopub.execute_input":"2025-03-16T11:14:29.522603Z","iopub.status.idle":"2025-03-16T11:14:29.529073Z","shell.execute_reply.started":"2025-03-16T11:14:29.522568Z","shell.execute_reply":"2025-03-16T11:14:29.528080Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# # captions on the validation set\n# rid = np.random.randint(0, len(img_name_val))\n# image = img_name_val[rid]\n# real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n# result, attention_plot = evaluate(image)\n\n# print ('Real Caption:', real_caption)\n# print ('Prediction Caption:', ' '.join(result))\n# plot_attention(image, result, attention_plot)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T08:29:03.781039Z","iopub.execute_input":"2025-03-16T08:29:03.781375Z","iopub.status.idle":"2025-03-16T08:29:03.785584Z","shell.execute_reply.started":"2025-03-16T08:29:03.781347Z","shell.execute_reply":"2025-03-16T08:29:03.784494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"real_captions = []\npred_captions = []\n\nwith open('./test_captions.csv', 'w') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"filename\",\"true_caption\", \"pred_caption\", \"gen_atten_weights\"])\n\n    for idx in tqdm(range(len(img_name_test))):\n        filename = os.path.basename(img_name_test[idx])\n        r_cap = [tokenizer.index_word[i] for i in cap_test[idx] if i not in [0]][1:-1]\n        p_cap,aw_cap = evaluate_without_plot(img_name_test[idx])\n    \n        real_captions.append(r_cap)\n        pred_captions.append(p_cap)\n\n        writer.writerow([filename,' '.join(r_cap), ' '.join(p_cap), aw_cap])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:14:33.051490Z","iopub.execute_input":"2025-03-16T11:14:33.051836Z","iopub.status.idle":"2025-03-16T11:24:43.382435Z","shell.execute_reply.started":"2025-03-16T11:14:33.051805Z","shell.execute_reply":"2025-03-16T11:24:43.381555Z"}},"outputs":[{"name":"stderr","text":" 34%|███▎      | 1006/3000 [03:24<07:04,  4.70it/s]Cleanup called...\n100%|██████████| 3000/3000 [10:10<00:00,  4.92it/s]\n","output_type":"stream"}],"execution_count":52}]}